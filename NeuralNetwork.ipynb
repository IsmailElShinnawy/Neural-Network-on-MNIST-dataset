{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "class Network():\n",
    "    \n",
    "    def __init__(self, layers, weights = None, biases = None):\n",
    "        self.num_layers = len(layers)\n",
    "        self.layers = layers\n",
    "        if weights and biases:\n",
    "            self.weights = weights\n",
    "            self.biases = biases\n",
    "        else:\n",
    "            # initializes the weights using Xavier initialization\n",
    "            self.weights = [np.random.uniform(-1, 1, (r, c))/np.sqrt(c) for r, c in zip(self.layers[1:], self.layers[:-1])]\n",
    "            self.biases = [np.random.uniform(-1, 1, (r, 1)) for r in self.layers[1:]]\n",
    "            \n",
    "        # momentum initialization  \n",
    "        self.v_delta_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        self.v_delta_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # RMS propagation initialization\n",
    "        self.s_delta_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        self.s_delta_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            a = np.tanh(np.dot(w, a)+b)\n",
    "        a = self.softmax(np.dot(self.weights[-1], a)+self.biases[-1])\n",
    "#         a = self.sigmoids(np.dot(self.weights[-1], a)+self.biases[-1])\n",
    "        return a\n",
    "                \n",
    "    def sigmoid(self, z):\n",
    "        return 1.0/(1+np.exp(-z))\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        s  = np.squeeze(np.sum(np.exp(z), axis = 0))\n",
    "        return np.exp(z)/s\n",
    "    \n",
    "    def fit(self, X_train, y_train, epochs, learning_rate, mini_batch_size, X_test, y_test, lmbda = 0, beta1 = 0, beta2 = 0, epsilon = 1e-8, print_validation_accuracy = False, print_validation_cost = False):\n",
    "        \n",
    "        validation_costs = []\n",
    "        validation_accuracies = []\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            \n",
    "            #shuffling the training data\n",
    "            permutation = list(np.random.permutation(X_train.shape[0]))\n",
    "            X_shuffled_train = X_train[permutation, :]\n",
    "            y_shuffled_train = y_train[permutation, :]\n",
    "            \n",
    "            #creating mini-batches\n",
    "            mini_batches_X = [X_shuffled_train[j:j+mini_batch_size] for j in range(0, len(X_train), mini_batch_size)]\n",
    "            mini_batches_y = [y_shuffled_train[j:j+mini_batch_size] for j in range(0, len(X_train), mini_batch_size)]\n",
    "            \n",
    "            for mini_batch_X, mini_batch_y in zip(mini_batches_X, mini_batches_y):\n",
    "                self.update_mini_batch(mini_batch_X, mini_batch_y, len(X_train), mini_batch_size, learning_rate, lmbda, beta1, beta2, epsilon)\n",
    "            \n",
    "            validation_accuracy, validation_cost = self.evaluate(X_test, y_test)\n",
    "            print(\"Epoch {}/{} done: {}/{}\".format(i+1, epochs, validation_accuracy, len(X_test)))\n",
    "            \n",
    "            if print_validation_accuracy:\n",
    "                validation_accuracies.append(validation_accuracy/len(X_test))\n",
    "            if print_validation_cost:\n",
    "                validation_costs.append(validation_cost)\n",
    "                \n",
    "        if print_validation_accuracy:\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(validation_accuracies)\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.xlabel('epochs (per fives)')\n",
    "            plt.show()\n",
    "        if print_validation_cost:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(validation_costs)\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('epochs (per fives)')\n",
    "            plt.show()\n",
    "                \n",
    "    def update_mini_batch(self, mini_batch_X, mini_batch_y, training_data_size, mini_batch_size, learning_rate, lmbda, beta1, beta2, epsilon):\n",
    "#         delta_nabla_b, delta_nabla_w = self.backprob(mini_batch_X.T, mini_batch_y.T, mini_batch_size, beta1, beta2)\n",
    "\n",
    "        # get updated values for v_delta and s_delta to update the network's learnable parameters\n",
    "        self.backprob(mini_batch_X.T, mini_batch_y.T, mini_batch_size, beta1, beta2)\n",
    "        \n",
    "        #bias correction for some reason throws an error\n",
    "#         v_delta_b = v_delta_b / (1-beta1**t)\n",
    "#         v_delta_w = v_delta_w / (1-beta1**t)\n",
    "        \n",
    "#         s_delta_b = s_delta_b / (1-beta2**t)\n",
    "#         s_delta_w = s_delta_w / (1-beta2**t)\n",
    "        \n",
    "        self.biases = [b-(learning_rate/mini_batch_size)*(vdb/np.sqrt(sdb+epsilon)) \n",
    "                        for b, vdb, sdb in zip(self.biases, self.v_delta_b, self.s_delta_b)]\n",
    "        self.weights = [(1-(learning_rate*lmbda/training_data_size))*w-(learning_rate/mini_batch_size)*(vdw/np.sqrt(sdw+epsilon)) \n",
    "                        for w, vdw, sdw in zip(self.weights, self.v_delta_w, self.s_delta_w)]\n",
    "        \n",
    "    def backprob(self, X, y, mini_batch_size, beta1, beta2):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        #forward pass\n",
    "        Z1 = np.dot(self.weights[0], X)+self.biases[0]\n",
    "        A1 = np.tanh(Z1)\n",
    "        Z2 = np.dot(self.weights[1], A1)+self.biases[1]\n",
    "        A2 = self.softmax(Z2)\n",
    "#         A2 = self.sigmoid(Z2)\n",
    "        \n",
    "        #backward pass\n",
    "        dZ2 = A2-y\n",
    "        \n",
    "        nabla_b[-1] = np.sum(dZ2, axis = 1, keepdims = True)/mini_batch_size\n",
    "        self.v_delta_b[-1] = beta1*self.v_delta_b[-1]+(1-beta1)*nabla_b[-1] #momentum\n",
    "        self.s_delta_b[-1] = beta2*self.s_delta_b[-1]+(1-beta2)*(nabla_b[-1]**2) #RMS\n",
    "        \n",
    "        nabla_w[-1] = np.dot(dZ2, A1.T)/mini_batch_size\n",
    "        self.v_delta_w[-1] = beta1*self.v_delta_w[-1]+(1-beta1)*nabla_w[-1]\n",
    "        self.s_delta_w[-1] = beta2*self.s_delta_w[-1]+(1-beta2)*(nabla_w[-1]**2)\n",
    "        \n",
    "        dZ1 = np.dot(self.weights[-1].T, dZ2)*(1-np.power(np.tanh(Z1), 2))\n",
    "        \n",
    "        nabla_b[-2] = np.sum(dZ1, axis = 1, keepdims = True)/mini_batch_size\n",
    "        self.v_delta_b[-2] = beta1*self.v_delta_b[-2]+(1-beta1)*nabla_b[-2]\n",
    "        self.s_delta_b[-2] = beta2*self.s_delta_b[-2]+(1-beta2)*(nabla_b[-2]**2)\n",
    "        \n",
    "        nabla_w[-2] = np.dot(dZ1, X.T)/mini_batch_size\n",
    "        self.v_delta_w[-2] = beta1*self.v_delta_w[-2]+(1-beta1)*nabla_w[-2]\n",
    "        self.s_delta_w[-2] = beta2*self.s_delta_w[-2]+(1-beta2)*(nabla_w[-2]**2)\n",
    "        \n",
    "#         return (nabla_b, nabla_w) #, v_delta_b, v_delta_w, s_delta_b, s_delta_w)\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        Z1 = np.dot(self.weights[0], X_test.T)+self.biases[0]\n",
    "        A1 = np.tanh(Z1)\n",
    "        Z2 = np.dot(self.weights[1], A1)+self.biases[1]\n",
    "        A2 = self.softmax(Z2)\n",
    "#         A2 = self.sigmoid(Z2)\n",
    "\n",
    "        res = np.argmax(A2, axis = 0)\n",
    "        cost = -np.sum(y_test.T*np.log(A2)+(1-y_test.T)*np.log(1-A2))/len(X_test)\n",
    "        validation_accuracy = sum(int(y_hat==y) for y_hat, y in zip(res, np.argmax(y_test.T, axis = 0)))\n",
    "        return (validation_accuracy, cost)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        res = self.feedforward(X.reshape((self.layers[0], 1)))\n",
    "        print(res)\n",
    "        return np.argmax(res, axis = 0)\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.resize(X_train.shape[0], X_train.shape[1]*X_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.resize(X_test.shape[0], X_test.shape[1]*X_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_changed = np.array([])\n",
    "y_train_changed = []\n",
    "for i in range(y_train.shape[0]):\n",
    "    y = [0]*10\n",
    "    y[y_train[i]] = 1\n",
    "    y_train_changed.append(y)\n",
    "y_train = np.array(y_train_changed)\n",
    "# print(y_train_changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_changed = []\n",
    "for i in range(y_test.shape[0]):\n",
    "    y = [0]*10\n",
    "    y[y_test[i]] = 1\n",
    "    y_test_changed.append(y)\n",
    "y_test = np.array(y_test_changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 done: 9460/10000\n",
      "Epoch 2/30 done: 9517/10000\n",
      "Epoch 3/30 done: 9564/10000\n",
      "Epoch 4/30 done: 9606/10000\n",
      "Epoch 5/30 done: 9581/10000\n",
      "Epoch 6/30 done: 9619/10000\n",
      "Epoch 7/30 done: 9611/10000\n",
      "Epoch 8/30 done: 9632/10000\n",
      "Epoch 9/30 done: 9619/10000\n",
      "Epoch 10/30 done: 9631/10000\n",
      "Epoch 11/30 done: 9612/10000\n",
      "Epoch 12/30 done: 9625/10000\n",
      "Epoch 13/30 done: 9598/10000\n",
      "Epoch 14/30 done: 9624/10000\n",
      "Epoch 15/30 done: 9625/10000\n",
      "Epoch 16/30 done: 9613/10000\n",
      "Epoch 17/30 done: 9615/10000\n",
      "Epoch 18/30 done: 9624/10000\n",
      "Epoch 19/30 done: 9600/10000\n",
      "Epoch 20/30 done: 9626/10000\n",
      "Epoch 21/30 done: 9617/10000\n",
      "Epoch 22/30 done: 9618/10000\n",
      "Epoch 23/30 done: 9634/10000\n",
      "Epoch 24/30 done: 9609/10000\n",
      "Epoch 25/30 done: 9619/10000\n",
      "Epoch 26/30 done: 9618/10000\n",
      "Epoch 27/30 done: 9620/10000\n",
      "Epoch 28/30 done: 9603/10000\n",
      "Epoch 29/30 done: 9616/10000\n",
      "Epoch 30/30 done: 9622/10000\n"
     ]
    }
   ],
   "source": [
    "nn = Network([784, 30, 10])\n",
    "nn.fit(X_train, y_train, 30, 0.01, 10, X_test, y_test, lmbda = 4, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, print_validation_accuracy = False, print_validation_cost = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(X_train[100].reshape((1, X_train[100].shape[0])))\n",
    "index = 1091\n",
    "img = X_test[index].reshape((28, 28))\n",
    "print(y_test[index])\n",
    "plt.imshow(img, cmap = \"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn.predict(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(nn.weights, open(\"weights/weights-0.01-eta-10-mbs-6-lmbda-default_adam-96.34-val_acc.pickle\", \"wb\"))\n",
    "pickle.dump(nn.biases, open(\"biases/biases-0.01-eta-10-mbs-6-lmbda-default_adam-96.34-val_acc.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "weights_trained = pickle.load(open(\"weights/weights-0.01-eta-10-mbs-6-lmbda-default_adam-96.34-val_acc.pickle\", \"rb\"))\n",
    "biases_trained = pickle.load(open(\"biases/biases-0.01-eta-10-mbs-6-lmbda-default_adam-96.34-val_acc.pickle\", \"rb\"))\n",
    "nn_trained = Network([784, 30, 10], weights = weights_trained, biases = biases_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1212\n",
    "img = X_test[index].reshape((28, 28))\n",
    "print(\"label: \", y_test[index])\n",
    "plt.imshow(img)#, cmap = \"binary\")\n",
    "# plt.show()\n",
    "print(\"prediction: \", nn_trained.predict(img))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bit90c94f1d101248888ddc5631c99b9813"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
