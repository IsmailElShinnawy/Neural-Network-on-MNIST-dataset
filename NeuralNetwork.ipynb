{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "class Network():\n",
    "    \n",
    "    def __init__(self, layers, weights = None, biases = None):\n",
    "        self.num_layers = len(layers)\n",
    "        self.layers = layers\n",
    "        if weights and biases:\n",
    "            self.weights = weights\n",
    "            self.biases = biases\n",
    "        else:\n",
    "            # initializes the weights using Xavier initialization\n",
    "            self.weights = [np.random.uniform(-1, 1, (r, c))/np.sqrt(c) for r, c in zip(self.layers[1:], self.layers[:-1])]\n",
    "            self.biases = [np.random.uniform(-1, 1, (r, 1)) for r in self.layers[1:]]\n",
    "            \n",
    "        # momentum initialization  \n",
    "        self.v_delta_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        self.v_delta_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # RMS propagation initialization\n",
    "        self.s_delta_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        self.s_delta_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            a = np.tanh(np.dot(w, a)+b)\n",
    "        a = self.softmax(np.dot(self.weights[-1], a)+self.biases[-1])\n",
    "#         a = self.sigmoids(np.dot(self.weights[-1], a)+self.biases[-1])\n",
    "        return a\n",
    "                \n",
    "    def sigmoid(self, z):\n",
    "        return 1.0/(1+np.exp(-z))\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        s  = np.squeeze(np.sum(np.exp(z), axis = 0))\n",
    "        return np.exp(z)/s\n",
    "    \n",
    "    def fit(self, X_train, y_train, epochs, learning_rate, mini_batch_size, X_test, y_test, lmbda = 0, beta1 = 0, beta2 = 0, epsilon = 1e-8, print_validation_accuracy = False, print_validation_cost = False):\n",
    "        \n",
    "        validation_costs = []\n",
    "        validation_accuracies = []\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            \n",
    "            #shuffling the training data\n",
    "            permutation = list(np.random.permutation(X_train.shape[0]))\n",
    "            X_shuffled_train = X_train[permutation, :]\n",
    "            y_shuffled_train = y_train[permutation, :]\n",
    "            \n",
    "            #creating mini-batches\n",
    "            mini_batches_X = [X_shuffled_train[j:j+mini_batch_size] for j in range(0, len(X_train), mini_batch_size)]\n",
    "            mini_batches_y = [y_shuffled_train[j:j+mini_batch_size] for j in range(0, len(X_train), mini_batch_size)]\n",
    "            \n",
    "            for mini_batch_X, mini_batch_y in zip(mini_batches_X, mini_batches_y):\n",
    "                self.update_mini_batch(mini_batch_X, mini_batch_y, len(X_train), mini_batch_size, learning_rate, lmbda, beta1, beta2, epsilon)\n",
    "            \n",
    "            validation_accuracy, validation_cost = self.evaluate(X_test, y_test)\n",
    "            print(\"Epoch {}/{} done: {}/{}\".format(i+1, epochs, validation_accuracy, len(X_test)))\n",
    "            \n",
    "            if print_validation_accuracy:\n",
    "                validation_accuracies.append(validation_accuracy/len(X_test))\n",
    "            if print_validation_cost:\n",
    "                validation_costs.append(validation_cost)\n",
    "                \n",
    "        if print_validation_accuracy:\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(validation_accuracies)\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.xlabel('epochs (per fives)')\n",
    "            plt.show()\n",
    "        if print_validation_cost:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(validation_costs)\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('epochs (per fives)')\n",
    "            plt.show()\n",
    "                \n",
    "    def update_mini_batch(self, mini_batch_X, mini_batch_y, training_data_size, mini_batch_size, learning_rate, lmbda, beta1, beta2, epsilon):\n",
    "\n",
    "        # get updated values for v_delta and s_delta to update the network's learnable parameters\n",
    "        self.backprob(mini_batch_X.T, mini_batch_y.T, mini_batch_size, beta1, beta2)\n",
    "        \n",
    "        self.biases = [b-(learning_rate/mini_batch_size)*(vdb/np.sqrt(sdb+epsilon)) \n",
    "                        for b, vdb, sdb in zip(self.biases, self.v_delta_b, self.s_delta_b)]\n",
    "        self.weights = [(1-(learning_rate*lmbda/training_data_size))*w-(learning_rate/mini_batch_size)*(vdw/np.sqrt(sdw+epsilon)) \n",
    "                        for w, vdw, sdw in zip(self.weights, self.v_delta_w, self.s_delta_w)]\n",
    "        \n",
    "    def backprob(self, X, y, mini_batch_size, beta1, beta2):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        #forward pass\n",
    "        Z1 = np.dot(self.weights[0], X)+self.biases[0]\n",
    "        A1 = np.tanh(Z1)\n",
    "        Z2 = np.dot(self.weights[1], A1)+self.biases[1]\n",
    "        A2 = self.softmax(Z2)\n",
    "#         A2 = self.sigmoid(Z2)\n",
    "        \n",
    "        #backward pass\n",
    "        dZ2 = A2-y\n",
    "        \n",
    "        nabla_b[-1] = np.sum(dZ2, axis = 1, keepdims = True)/mini_batch_size\n",
    "        self.v_delta_b[-1] = beta1*self.v_delta_b[-1]+(1-beta1)*nabla_b[-1] #momentum\n",
    "        self.s_delta_b[-1] = beta2*self.s_delta_b[-1]+(1-beta2)*(nabla_b[-1]**2) #RMS\n",
    "        \n",
    "        nabla_w[-1] = np.dot(dZ2, A1.T)/mini_batch_size\n",
    "        self.v_delta_w[-1] = beta1*self.v_delta_w[-1]+(1-beta1)*nabla_w[-1]\n",
    "        self.s_delta_w[-1] = beta2*self.s_delta_w[-1]+(1-beta2)*(nabla_w[-1]**2)\n",
    "        \n",
    "        dZ1 = np.dot(self.weights[-1].T, dZ2)*(1-np.power(np.tanh(Z1), 2))\n",
    "        \n",
    "        nabla_b[-2] = np.sum(dZ1, axis = 1, keepdims = True)/mini_batch_size\n",
    "        self.v_delta_b[-2] = beta1*self.v_delta_b[-2]+(1-beta1)*nabla_b[-2]\n",
    "        self.s_delta_b[-2] = beta2*self.s_delta_b[-2]+(1-beta2)*(nabla_b[-2]**2)\n",
    "        \n",
    "        nabla_w[-2] = np.dot(dZ1, X.T)/mini_batch_size\n",
    "        self.v_delta_w[-2] = beta1*self.v_delta_w[-2]+(1-beta1)*nabla_w[-2]\n",
    "        self.s_delta_w[-2] = beta2*self.s_delta_w[-2]+(1-beta2)*(nabla_w[-2]**2)\n",
    "            \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        Z1 = np.dot(self.weights[0], X_test.T)+self.biases[0]\n",
    "        A1 = np.tanh(Z1)\n",
    "        Z2 = np.dot(self.weights[1], A1)+self.biases[1]\n",
    "        A2 = self.softmax(Z2)\n",
    "#         A2 = self.sigmoid(Z2)\n",
    "\n",
    "        res = np.argmax(A2, axis = 0)\n",
    "        cost = -np.sum(y_test.T*np.log(A2)+(1-y_test.T)*np.log(1-A2))/len(X_test)\n",
    "        validation_accuracy = sum(int(y_hat==y) for y_hat, y in zip(res, np.argmax(y_test.T, axis = 0)))\n",
    "        return (validation_accuracy, cost)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        res = self.feedforward(X.reshape((self.layers[0], 1)))\n",
    "        print(res)\n",
    "        return np.argmax(res, axis = 0)\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.resize(X_train.shape[0], X_train.shape[1]*X_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.resize(X_test.shape[0], X_test.shape[1]*X_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_changed = []\n",
    "for i in range(y_train.shape[0]):\n",
    "    y = [0]*10\n",
    "    y[y_train[i]] = 1\n",
    "    y_train_changed.append(y)\n",
    "y_train = np.array(y_train_changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_changed = []\n",
    "for i in range(y_test.shape[0]):\n",
    "    y = [0]*10\n",
    "    y[y_test[i]] = 1\n",
    "    y_test_changed.append(y)\n",
    "y_test = np.array(y_test_changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 done: 9495/10000\n",
      "Epoch 2/30 done: 9549/10000\n",
      "Epoch 3/30 done: 9551/10000\n",
      "Epoch 4/30 done: 9607/10000\n",
      "Epoch 5/30 done: 9646/10000\n",
      "Epoch 6/30 done: 9619/10000\n",
      "Epoch 7/30 done: 9628/10000\n",
      "Epoch 8/30 done: 9625/10000\n",
      "Epoch 9/30 done: 9659/10000\n",
      "Epoch 10/30 done: 9576/10000\n",
      "Epoch 11/30 done: 9622/10000\n",
      "Epoch 12/30 done: 9614/10000\n",
      "Epoch 13/30 done: 9601/10000\n",
      "Epoch 14/30 done: 9627/10000\n",
      "Epoch 15/30 done: 9610/10000\n",
      "Epoch 16/30 done: 9626/10000\n",
      "Epoch 17/30 done: 9589/10000\n",
      "Epoch 18/30 done: 9596/10000\n",
      "Epoch 19/30 done: 9626/10000\n",
      "Epoch 20/30 done: 9627/10000\n",
      "Epoch 21/30 done: 9610/10000\n",
      "Epoch 22/30 done: 9599/10000\n",
      "Epoch 23/30 done: 9606/10000\n",
      "Epoch 24/30 done: 9606/10000\n",
      "Epoch 25/30 done: 9569/10000\n",
      "Epoch 26/30 done: 9600/10000\n",
      "Epoch 27/30 done: 9599/10000\n",
      "Epoch 28/30 done: 9597/10000\n",
      "Epoch 29/30 done: 9607/10000\n",
      "Epoch 30/30 done: 9585/10000\n"
     ]
    }
   ],
   "source": [
    "nn = Network([784, 30, 10])\n",
    "nn.fit(X_train, y_train, 30, 0.01, 10, X_test, y_test, lmbda = 4, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, print_validation_accuracy = False, print_validation_cost = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(nn.weights, open(\"weights/weights-0.01-eta-10-mbs-6-lmbda-default_adam-96.34-val_acc.pickle\", \"wb\"))\n",
    "pickle.dump(nn.biases, open(\"biases/biases-0.01-eta-10-mbs-6-lmbda-default_adam-96.34-val_acc.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "weights_trained = pickle.load(open(\"weights/weights-0.01-eta-10-mbs-6-lmbda-default_adam-96.34-val_acc.pickle\", \"rb\"))\n",
    "biases_trained = pickle.load(open(\"biases/biases-0.01-eta-10-mbs-6-lmbda-default_adam-96.34-val_acc.pickle\", \"rb\"))\n",
    "nn_trained = Network([784, 30, 10], weights = weights_trained, biases = biases_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  [0 0 0 0 0 0 1 0 0 0]\n",
      "[[6.39317327e-06]\n",
      " [4.39561771e-09]\n",
      " [8.81346605e-06]\n",
      " [1.99474529e-07]\n",
      " [7.68780073e-04]\n",
      " [2.98163676e-05]\n",
      " [9.96421174e-01]\n",
      " [3.48072202e-08]\n",
      " [2.76298107e-03]\n",
      " [1.80355231e-06]]\n",
      "prediction:  [6]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 1212\n",
    "img = X_test[index].reshape((28, 28))\n",
    "print(\"label: \", y_test[index])\n",
    "plt.imshow(img)#, cmap = \"binary\")\n",
    "print(\"prediction: \", nn_trained.predict(img))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bit90c94f1d101248888ddc5631c99b9813"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
