{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Network():\n",
    "    \n",
    "    def __init__(self, layers, weights = None, biases = None):\n",
    "        self.num_layers = len(layers)\n",
    "        self.layers = layers\n",
    "        if weights and biases:\n",
    "            self.weights = weights\n",
    "            self.biases = biases\n",
    "        else:\n",
    "            self.weights = [np.random.uniform(-1, 1, (r, c))/np.sqrt(c) for r, c in zip(self.layers[1:], self.layers[:-1])]\n",
    "            self.biases = [np.random.uniform(-1, 1, (r, 1)) for r in self.layers[1:]]\n",
    "            \n",
    "        self.v_delta_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        self.v_delta_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        \n",
    "        self.s_delta_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        self.s_delta_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            a = np.tanh(np.dot(w, a)+b)\n",
    "        a = self.softmax(np.dot(self.weights[-1], a)+self.biases[-1])\n",
    "#         a = self.sigmoids(np.dot(self.weights[-1], a)+self.biases[-1])\n",
    "        return a    \n",
    "                \n",
    "    def sigmoid(self, z):\n",
    "        return 1.0/(1+np.exp(-z))\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        s  = np.squeeze(np.sum(np.exp(z), axis = 0))\n",
    "        return np.exp(z)/s\n",
    "    \n",
    "    def SGD(self, X_train, y_train, epochs, learning_rate, mini_batch_size, X_test, y_test, lmbda = 0, beta1 = 0, beta2 = 0, epsilon = 1e-8):\n",
    "        for i in range(epochs):\n",
    "            permutation = list(np.random.permutation(X_train.shape[0]))\n",
    "            X_shuffled_train = X_train[permutation, :]\n",
    "            y_shuffled_train = y_train[permutation, :]\n",
    "            mini_batches_X = [X_shuffled_train[j:j+mini_batch_size] for j in range(0, len(X_train), mini_batch_size)]\n",
    "            mini_batches_y = [y_shuffled_train[j:j+mini_batch_size] for j in range(0, len(X_train), mini_batch_size)]\n",
    "            for mini_batch_X, mini_batch_y in zip(mini_batches_X, mini_batches_y):\n",
    "                self.update_mini_batch(mini_batch_X, mini_batch_y, len(X_train), mini_batch_size, learning_rate, lmbda, beta1, beta2, epsilon)\n",
    "            print(\"Epoch {}/{} done: {}/{}\".format(i+1, epochs, self.evaluate(X_test, y_test), len(X_test)))\n",
    "                \n",
    "    def update_mini_batch(self, mini_batch_X, mini_batch_y, training_data_size, mini_batch_size, learning_rate, lmbda, beta1, beta2, epsilon):\n",
    "        \n",
    "        delta_nabla_b, delta_nabla_w = self.backprob(mini_batch_X.T, mini_batch_y.T, mini_batch_size, beta1, beta2)\n",
    "        \n",
    "        self.biases = [b-(learning_rate/mini_batch_size)*(vdb/np.sqrt(sdb+epsilon)) \n",
    "                       for b, vdb, sdb in zip(self.biases, self.v_delta_b, self.s_delta_b)]\n",
    "        self.weights = [(1-(learning_rate*lmbda/training_data_size))*w-(learning_rate/mini_batch_size)*(vdw/np.sqrt(sdw+epsilon)) \n",
    "                        for w, vdw, sdw in zip(self.weights, self.v_delta_w, self.s_delta_w)]\n",
    "        \n",
    "    def backprob(self, X, y, mini_batch_size, beta1, beta2):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        activation = X\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = np.tanh(z)\n",
    "            activations.append(activation)\n",
    "            \n",
    "        #last layer uses sigmoid/softmax activation not tanh\n",
    "        z = np.dot(self.weights[-1], activations[-1])+self.biases[-1]\n",
    "        zs.append(z)\n",
    "        activation = self.sigmoid(z)\n",
    "#         activation = self.softmax(z)\n",
    "        activations.append(activation)\n",
    "        \n",
    "        delta = activations[-1]-y\n",
    "        \n",
    "        nabla_b[-1] = np.sum(delta, axis = 1, keepdims = True)/mini_batch_size\n",
    "        self.v_delta_b[-1] = beta1*self.v_delta_b[-1]+(1-beta1)*nabla_b[-1]\n",
    "        self.s_delta_b[-1] = beta2*self.s_delta_b[-1]+(1-beta2)*(nabla_b[-1]**2)\n",
    "        \n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].T)/mini_batch_size\n",
    "        self.v_delta_w[-1] = beta1*self.v_delta_w[-1]+(1-beta1)*nabla_w[-1]\n",
    "        self.s_delta_w[-1] = beta2*self.s_delta_w[-1]+(1-beta2)*(nabla_w[-1]**2)\n",
    "        \n",
    "        for l in range(2, self.num_layers):\n",
    "            delta = np.dot(self.weights[-l+1].T, delta)*(1-np.power(np.tanh(zs[-l]), 2))\n",
    "            \n",
    "            nabla_b[-l] = np.sum(delta, axis = 1, keepdims = True)/mini_batch_size\n",
    "            self.v_delta_b[-l] = beta1*self.v_delta_b[-l]+(1-beta1)*nabla_b[-l]\n",
    "            self.s_delta_b[-l] = beta2*self.s_delta_b[-l]+(1-beta2)*(nabla_b[-l]**2)\n",
    "            \n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].T)/mini_batch_size\n",
    "            self.v_delta_w[-l] = beta1*self.v_delta_w[-l]+(1-beta1)*nabla_w[-l]\n",
    "            self.s_delta_w[-l] = beta2*self.s_delta_w[-l]+(1-beta2)*(nabla_w[-l]**2)\n",
    "            \n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "\n",
    "        activation = X_test.T\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            activation = np.tanh(np.dot(w, activation)+b)\n",
    "        activation = self.sigmoid(np.dot(self.weights[-1], activation)+self.biases[-1])\n",
    "        \n",
    "        res = np.argmax(activation, axis = 0)\n",
    "        return sum(int(y_hat==y) for y_hat, y in zip(res, y_test))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        res = self.feedforward(X.reshape((self.layers[0], 1)))\n",
    "        print(res)\n",
    "        return np.argmax(res, axis = 0)\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn =  Network([28*28, 32, 32, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 done: 9353/10000\n",
      "Epoch 2/30 done: 9472/10000\n",
      "Epoch 3/30 done: 9523/10000\n",
      "Epoch 4/30 done: 9559/10000\n",
      "Epoch 5/30 done: 9566/10000\n",
      "Epoch 6/30 done: 9566/10000\n",
      "Epoch 7/30 done: 9611/10000\n",
      "Epoch 8/30 done: 9605/10000\n",
      "Epoch 9/30 done: 9592/10000\n",
      "Epoch 10/30 done: 9614/10000\n",
      "Epoch 11/30 done: 9600/10000\n",
      "Epoch 12/30 done: 9606/10000\n",
      "Epoch 13/30 done: 9573/10000\n",
      "Epoch 14/30 done: 9599/10000\n",
      "Epoch 15/30 done: 9586/10000\n",
      "Epoch 16/30 done: 9592/10000\n",
      "Epoch 17/30 done: 9617/10000\n",
      "Epoch 18/30 done: 9605/10000\n",
      "Epoch 19/30 done: 9608/10000\n",
      "Epoch 20/30 done: 9617/10000\n",
      "Epoch 21/30 done: 9577/10000\n",
      "Epoch 22/30 done: 9576/10000\n",
      "Epoch 23/30 done: 9626/10000\n",
      "Epoch 24/30 done: 9618/10000\n",
      "Epoch 25/30 done: 9591/10000\n",
      "Epoch 26/30 done: 9597/10000\n",
      "Epoch 27/30 done: 9588/10000\n",
      "Epoch 28/30 done: 9615/10000\n",
      "Epoch 29/30 done: 9601/10000\n",
      "Epoch 30/30 done: 9564/10000\n"
     ]
    }
   ],
   "source": [
    "dnn.SGD(X_train, y_train, 30, 0.01, 10, X_test, y_test, lmbda = 6, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.resize(X_train.shape[0], X_train.shape[1]*X_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.resize(X_test.shape[0], X_test.shape[1]*X_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_changed = []\n",
    "for i in range(y_train.shape[0]):\n",
    "    y = [0]*10\n",
    "    y[y_train[i]] = 1\n",
    "    y_train_changed.append(y)\n",
    "y_train = np.array(y_train_changed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bit90c94f1d101248888ddc5631c99b9813"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
