{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Network():\n",
    "    \n",
    "    def __init__(self, layers, weights = None, biases = None):\n",
    "        self.num_layers = len(layers)\n",
    "        self.layers = layers\n",
    "        if weights and biases:\n",
    "            self.weights = weights\n",
    "            self.biases = biases\n",
    "        else:\n",
    "            self.weights = [np.random.uniform(-1, 1, (r, c))/np.sqrt(c) for r, c in zip(self.layers[1:], self.layers[:-1])]\n",
    "            self.biases = [np.random.uniform(-1, 1, (r, 1)) for r in self.layers[1:]]\n",
    "            \n",
    "        self.v_delta_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        self.v_delta_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        \n",
    "        self.s_delta_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        self.s_delta_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        for i in range(self.num_layers-1):\n",
    "            if i==self.num_layers-2:\n",
    "                a = self.sigmoid(np.dot(self.weights[i], a)+self.biases[i])\n",
    "#                 a = self.softmax(np.dot(self.weights[i], a)+self.biases[i])\n",
    "            else:\n",
    "                a = np.tanh(np.dot(self.weights[i], a)+self.biases[i])\n",
    "        return a\n",
    "                \n",
    "    def sigmoid(self, z):\n",
    "        return 1.0/(1+np.exp(-z))\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        s  = np.squeeze(np.sum(np.exp(z), axis = 0))\n",
    "        return np.exp(z)/s\n",
    "    \n",
    "    def SGD(self, X_train, y_train, epochs, learning_rate, mini_batch_size, X_test, y_test, lmbda = 0, beta1 = 0, beta2 = 0, epsilon = 1e-8):\n",
    "        for i in range(epochs):\n",
    "            permutation = list(np.random.permutation(X_train.shape[0]))\n",
    "            X_shuffled_train = X_train[permutation, :]\n",
    "            y_shuffled_train = y_train[permutation, :]\n",
    "            mini_batches_X = [X_shuffled_train[j:j+mini_batch_size] for j in range(0, len(X_train), mini_batch_size)]\n",
    "            mini_batches_y = [y_shuffled_train[j:j+mini_batch_size] for j in range(0, len(X_train), mini_batch_size)]\n",
    "            for mini_batch_X, mini_batch_y in zip(mini_batches_X, mini_batches_y):\n",
    "                self.update_mini_batch(mini_batch_X, mini_batch_y, len(X_train), mini_batch_size, learning_rate, lmbda, beta1, beta2, epsilon)\n",
    "            print(\"Epoch {}/{} done: {}/{}\".format(i+1, epochs, self.evaluate(X_test, y_test), len(X_test)))\n",
    "                \n",
    "    def update_mini_batch(self, mini_batch_X, mini_batch_y, training_data_size, mini_batch_size, learning_rate, lmbda, beta1, beta2, epsilon):\n",
    "        \n",
    "        delta_nabla_b, delta_nabla_w = self.backprob(mini_batch_X.T, mini_batch_y.T, mini_batch_size, beta1, beta2)\n",
    "        \n",
    "        self.biases = [b-(learning_rate/mini_batch_size)*(vdb/np.sqrt(sdb+epsilon)) \n",
    "                       for b, vdb, sdb in zip(self.biases, self.v_delta_b, self.s_delta_b)]\n",
    "        self.weights = [(1-(learning_rate*lmbda/training_data_size))*w-(learning_rate/mini_batch_size)*(vdw/np.sqrt(sdw+epsilon)) \n",
    "                        for w, vdw, sdw in zip(self.weights, self.v_delta_w, self.s_delta_w)]\n",
    "        \n",
    "    def backprob(self, X, y, mini_batch_size, beta1, beta2):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        activation = X\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = np.tanh(z)\n",
    "            activations.append(activation)\n",
    "            \n",
    "        #last layer uses sigmoid/softmax activation not tanh\n",
    "        z = np.dot(self.weights[-1], activations[-1])+self.biases[-1]\n",
    "        zs.append(z)\n",
    "        activation = self.sigmoid(z)\n",
    "#         activation = self.softmax(z)\n",
    "        activations.append(activation)\n",
    "        \n",
    "#         print(activations[-1].shape)\n",
    "        \n",
    "        delta = activations[-1]-y\n",
    "        \n",
    "        nabla_b[-1] = np.sum(delta, axis = 1, keepdims = True)/mini_batch_size\n",
    "        self.v_delta_b[-1] = beta1*self.v_delta_b[-1]+(1-beta1)*nabla_b[-1]\n",
    "        self.s_delta_b[-1] = beta2*self.s_delta_b[-1]+(1-beta2)*(nabla_b[-1]**2)\n",
    "        \n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].T)/mini_batch_size\n",
    "        self.v_delta_w[-1] = beta1*self.v_delta_w[-1]+(1-beta1)*nabla_w[-1]\n",
    "        self.s_delta_w[-1] = beta2*self.s_delta_w[-1]+(1-beta2)*(nabla_w[-1]**2)\n",
    "        \n",
    "        for l in range(2, self.num_layers):\n",
    "            delta = np.dot(self.weights[-l+1].T, delta)*(1-np.power(np.tanh(zs[-l]), 2))\n",
    "            \n",
    "            nabla_b[-l] = np.sum(delta, axis = 1, keepdims = True)/mini_batch_size\n",
    "            self.v_delta_b[-l] = beta1*self.v_delta_b[-l]+(1-beta1)*nabla_b[-l]\n",
    "            self.s_delta_b[-l] = beta2*self.s_delta_b[-l]+(1-beta2)*(nabla_b[-l]**2)\n",
    "            \n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].T)/mini_batch_size\n",
    "            self.v_delta_w[-l] = beta1*self.v_delta_w[-l]+(1-beta1)*nabla_w[-l]\n",
    "            self.s_delta_w[-l] = beta2*self.s_delta_w[-l]+(1-beta2)*(nabla_w[-l]**2)\n",
    "            \n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "#         activation = X_test.T\n",
    "#         for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "#             z = np.dot(w, activation)+b\n",
    "#             activation = np.tanh(z)\n",
    "#         z = np.dot(self.weights[-1], activation)+self.biases[-1]\n",
    "#         activation = self.sigmoid(z)\n",
    "#         res = 0\n",
    "# #         print(activation.shape)\n",
    "#         for y_hat, y in zip(activation[0], y_test):\n",
    "#             if y_hat>0.5:\n",
    "#                 y_hat = 1\n",
    "#             else:\n",
    "#                 y_hat = 0\n",
    "#             res+=int(y_hat==y)\n",
    "#         return res\n",
    "\n",
    "        activation = X_test.T\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            activation = np.tanh(np.dot(w, activation)+b)\n",
    "        activation = self.sigmoid(np.dot(self.weights[-1], activation)+self.biases[-1])\n",
    "        \n",
    "        res = np.argmax(activation, axis = 0)\n",
    "        return sum(int(y_hat==y) for y_hat, y in zip(res, y_test))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        res = self.feedforward(X.reshape((self.layers[0], 1)))\n",
    "        print(res)\n",
    "        return np.argmax(res, axis = 0)\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "X = pickle.load(open(\"CatsAndDogs/X.pickle\", \"rb\"))\n",
    "y = pickle.load(open(\"CatsAndDogs/y.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.resize(X.shape[0], X.shape[1]*X.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn =  Network([28*28, 32, 32, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn.SGD(X_train, y_train, 30, 0.01, 10, X_test, y_test, lmbda = 6, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = Network([50*50, 32, 16, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_cutoff = 3000#2996\n",
    "dnn.SGD(X[:-validation_cutoff], y[:-validation_cutoff], 30, 0.01, 10, X[-validation_cutoff:], y[-validation_cutoff:], lmbda = 2, beta1 = 0.9, beta2 = 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "print(a[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.resize(X_train.shape[0], X_train.shape[1]*X_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.resize(X_test.shape[0], X_test.shape[1]*X_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_changed = []\n",
    "for i in range(y_train.shape[0]):\n",
    "    y = [0]*10\n",
    "    y[y_train[i]] = 1\n",
    "    y_train_changed.append(y)\n",
    "y_train = np.array(y_train_changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bit90c94f1d101248888ddc5631c99b9813"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
